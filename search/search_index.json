{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#add-line-numbers","title":"Add Line Numbers","text":"<pre><code>from fastapi import FastAPI, HTTPException\nimport requests\nimport uvicorn\n\napp = FastAPI()\n\n# Define the endpoint for the chat completion API\nAPI_ENDPOINT = \"http://localhost:11434/api/chat\"\n\n# Define your model name here\nMODEL_NAME = \"mixtral\"\n\n@app.post(\"/chat/\")\nasync def chat(message: str):\n</code></pre>"},{"location":"#highlight-code-lines","title":"Highlight Code Lines","text":"<pre><code>    if response.status_code != 200:\n        # Handle any errors from that ollama endpoint on my server\n        raise HTTPException(status_code=response.status_code, detail=\"Error from the chat API\")\n</code></pre>"},{"location":"#icons-and-emojis","title":"Icons and Emojis","text":""},{"location":"kubeadm/","title":"KubeAdm","text":""},{"location":"kubeadm/#kubeadm-install-script-singlenode-cluster","title":"KubeADM Install Script - SingleNode Cluster","text":"<p>Guides - Install KubeAdm on Multi-Node Cluster - Install KubeAdm on Single-Node</p> <p>Minimum Specs - 4GB RAM + 2 CPU's + 20GB Free Space </p>"},{"location":"kubeadm/#pre-requisites","title":"Pre-requisites","text":"<ol> <li>Ensure the name of Host is consistent with the <code>/etc/hosts</code> file you will modify below</li> <li>Default name is <code>k8s-control</code></li> </ol>"},{"location":"kubeadm/#install-general-dependencies","title":"Install General Dependencies","text":"<pre><code>sudo nano /etc/hostname\n# or\nsudo hostnamectl set-hostname \"k8s-cluster1\"\n</code></pre>"},{"location":"kubeadm/#disable-swap-and-add-kernal-parameters","title":"Disable Swap and Add Kernal Parameters","text":"<pre><code>sudo swapoff -a\n#comment last line out\nsudo nano /etc/fstab \n</code></pre> <pre><code># Write the lines overlay and br_netfilter to the .conf file\nsudo tee /etc/modules-load.d/containerd.conf &lt;&lt;EOF\noverlay\nbr_netfilter\nEOF\n\nmodprobe overlay\nmodprobe br_netfilter\n\n# Add kernal parameters\nsudo tee /etc/sysctl.d/kubernetes.conf &lt;&lt;EOF\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nEOF\n\n# Reload changes\nsudo sysctl --system\n</code></pre>"},{"location":"kubeadm/#install-containerd-runtime","title":"Install Containerd Runtime","text":"<p><code>sudo apt install -y curl gnupg2 software-properties-common apt-transport-https ca-certificates</code></p>"},{"location":"kubeadm/#enable-docker-repo-and-install-containerd","title":"Enable Docker Repo and Install Containerd","text":"<pre><code>sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmour -o /etc/apt/trusted.gpg.d/docker.gpg\n\nsudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\"\n\nsudo apt update\n\nsudo apt install -y containerd.io\n\n# Start as cgroup w/ systemd\n\ncontainerd config default | sudo tee /etc/containerd/config.toml &gt;/dev/null 2&gt;&amp;1\n\nsudo sed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml\n\nsudo systemctl restart containerd\nsudo systemctl enable containerd\n</code></pre>"},{"location":"kubeadm/#add-apt-repository-for-kubernetes","title":"Add Apt Repository for Kubernetes","text":"<pre><code>curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmour -o /etc/apt/trusted.gpg.d/kubernetes-xenial.gpg\n\nsudo apt-add-repository \"deb http://apt.kubernetes.io/ kubernetes-xenial main\"\n\n# Install Kubectl, Kubeadm and Kubelet\nsudo apt update\nsudo apt install -y kubelet kubeadm kubectl\nsudo apt-mark hold kubelet kubeadm kubectl\n</code></pre>"},{"location":"kubeadm/#initialize-single-node-cluster","title":"Initialize Single-Node Cluster","text":"<p><code>sudo kubeadm init --pod-network-cidr=10.244.0.0/16</code></p> <p>Ensure you run the below commands <pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre></p>"},{"location":"kubeadm/#untaint-node-for-single-cluster-pod-deployments","title":"Untaint Node for Single-Cluster Pod Deployments","text":"<pre><code>kubectl taint nodes --all node-role.kubernetes.io/control-plane-\n</code></pre>"},{"location":"kubeadm/#install-calico-cni-plugin","title":"Install Calico CNI Plugin","text":"<p><code>kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml</code></p>"},{"location":"kubeadm/#simple-nginx-pod-test","title":"Simple Nginx Pod Test","text":"<pre><code>kubectl run nginx --image=nginx\nkubectl expose pod nginx --port=80 --type=NodePort\n\n# Alternate\nkubectl create deployment nginx-app --image=nginx --replicas=2\n\nkubectl expose deployment nginx-app --type=NodePort --port=80\n\ncurl http://192.168.1.230:&lt;NodePort&gt;\n</code></pre>"},{"location":"kubeadm/#installing-add-ons","title":"Installing Add-Ons","text":""},{"location":"kubeadm/#install-helm","title":"Install Helm","text":"<p><code>curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash</code></p>"},{"location":"kubeadm/#install-csi-driver-openbs","title":"Install CSI Driver - OpenBS","text":"<pre><code>helm repo add openebs https://openebs.github.io/charts\n\nkubectl create namespace openebs\n\nhelm --namespace=openebs install openebs openebs/openebs\n\n# Add bitnami repo to helm\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n\nhelm install wordpress bitnami/wordpress --set=global.storageClass=openebs-hostpath\n</code></pre> <p>Original Link</p>"},{"location":"kubeadm/#how-to-easily-setup-a-multi-node-kubernetes-cluster-on-a-single-machine-with-terraform-libvirt-and-fedora-coreos","title":"How to Easily Setup a Multi-node Kubernetes Cluster on a Single Machine with Terraform, libvirt and Fedora CoreOS","text":"<p>Luigi Quattrocchi Senior Software Engineer at TIM</p> <p>Published on December 28, 2022</p> <p>This article describes a simple procedure to setup a multi-node Kubernetes cluster for development/testing/learning purposes on a single Linux machine using Terraform, libvirt and Fedora CoreOS virtual machines.</p>"},{"location":"kubeadm/#requirements","title":"Requirements","text":"<p>On the target Linux machine the following software must be installed:</p> <ul> <li>libvirt library: On Fedora, the installation can be done using the following command:   <pre><code>sudo dnf group install --with-optional virtualization\n</code></pre></li> <li>Terraform: On Fedora, the installation can be done using these commands:   <pre><code>sudo dnf install -y dnf-plugins-core\nsudo dnf config-manager --add-repo https://rpm.releases.hashicorp.com/fedora/hashicorp.repo\nsudo dnf -y install terraform\n</code></pre></li> <li>Butane: On Fedora, the installation can be done using this command:   <pre><code>sudo dnf install -y butane\n</code></pre></li> </ul> <p>Clone the GitHub repository: <pre><code>git clone https://github.com/luigiqtt/dev-multinode-k8s.git\n</code></pre></p>"},{"location":"kubeadm/#configuration","title":"Configuration","text":""},{"location":"kubeadm/#terraform-variables","title":"Terraform Variables","text":"<p>Modify the <code>k8s.auto.tfvars</code> and <code>k8s.secret.auto.tfvars</code> files according to your requirements.</p>"},{"location":"kubeadm/#butane-files","title":"Butane Files","text":"<p>Modify the Butane YAML-formatted files for each node in the <code>config</code> directory as needed.</p>"},{"location":"kubeadm/#cluster-creation","title":"Cluster Creation","text":"<ol> <li>Initialize Terraform:    <pre><code>cd dev-multinode-k8s\nterraform init\n</code></pre></li> <li>Apply the configuration with Terraform:    <pre><code>sudo terraform apply\n</code></pre></li> <li>Initialize the Control Plane node:    <pre><code>ssh admin@192.168.40.162\ncd setup\n./init.sh\n</code></pre></li> <li>Copy the join command from the script execution log and initialize the worker nodes.</li> </ol>"},{"location":"kubeadm/#addremove-worker-nodes","title":"Add/Remove Worker Nodes","text":""},{"location":"kubeadm/#remove-worker-nodes","title":"Remove Worker Nodes","text":"<ol> <li>Modify the <code>worker_count</code> parameter in <code>k8s.auto.tfvars</code> file, setting a smaller value, then run the Terraform apply command: <pre><code>sudo terraform apply\n</code></pre> Note that in this way the virtual machines corresponding to the removed nodes will be destroyed and this could have some unwanted impacts on the running pods. See Safely Drain a Node for recommendations on how to properly remove a node from the cluster.</li> </ol>"},{"location":"kubeadm/#add-new-worker-nodes","title":"Add New Worker Nodes","text":"<ol> <li>Modify the <code>worker_count</code> parameter in <code>k8s.auto.tfvars</code>, setting a higher value.</li> <li>Create additional Butane files in the <code>config</code> folder for each new worker if required.</li> <li>Execute <code>sudo terraform apply</code> to apply the changes.</li> </ol>"},{"location":"kubeadm/#nodes-configuration-updates","title":"Nodes Configuration Updates","text":"<p>There are two ways to modify the nodes' configuration parameters:</p> <ol> <li>Using Terraform: Modify the parameters in the configuration files and execute <code>sudo terraform apply</code>.</li> <li>Using libvirt (Virtual Machine Manager or virsh command): This method is recommended if Terraform plans to destroy any resources.</li> </ol>"},{"location":"kubeadm/#example-changing-the-startup-behavior","title":"Example: Changing the Startup Behavior","text":"<ul> <li>To change the automatic startup of the nodes, modify the <code>autostart</code> parameter in <code>k8s.auto.tfvars</code>.</li> </ul>"},{"location":"kubeadm/#control-the-cluster-from-the-host-machine","title":"Control the Cluster from the Host Machine","text":"<ul> <li>Install <code>kubectl</code> on the host machine.</li> <li>Copy the <code>.kube/config</code> file from the Control Plane node to the host machine.</li> </ul>"},{"location":"kubeadm/#deploy-an-nginx-ingress-controller","title":"Deploy an NGINX Ingress Controller","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.6.4/deploy/static/provider/cloud/deploy.yaml\n</code></pre> <ul> <li>Check the status of the controller:   <pre><code>kubectl describe -n ingress-nginx deploy/ingress-nginx-controller\n</code></pre></li> </ul>"},{"location":"kubeadm/#deploy-and-access-the-kubernetes-dashboard","title":"Deploy and Access the Kubernetes Dashboard","text":"<ul> <li>To deploy and access the Kubernetes Dashboard, visit Kubernetes Dashboard Documentation.</li> </ul>"},{"location":"kubeadm/#destroy-the-cluster","title":"Destroy the Cluster","text":"<ul> <li>To destroy the cluster and remove all the created virtual machines:   <pre><code>sudo terraform destroy\n</code></pre></li> </ul>"},{"location":"kubeadm/#references","title":"References","text":"<ul> <li>Creating a Kubernetes Cluster with Fedora CoreOS 36</li> <li>Fedora CoreOS - Basic Kubernetes Setup</li> <li>Creating a cluster with kubeadm</li> </ul> <p>Published by Luigi Quattrocchi, Senior Software Engineer at TIM</p>"},{"location":"kubeadm/#tags","title":"Tags","text":"<p>#terraform #kubernetes #k8s #virtualization #libvirt #linux #fedora ```</p> <p>This Markdown representation provides a structured and readable format of the article, suitable for documentation or publishing in platforms that support Markdown.</p>"},{"location":"kubernetes/","title":"Kubernetes Therapy","text":""},{"location":"kubernetes/#purpose","title":"Purpose","text":"<p>A good way to find out how to deal with the intracacies of configuring and running Kubernetes On-Prem.</p>"},{"location":"kubernetes/#ckad-commands","title":"CKAD Commands","text":"<pre><code># General Commands\nkubectl &lt;action&gt; --help\nkubectl run &lt;podName&gt; --image=nginx --restart=Never --dry-run -o yaml &gt; output.yaml\nkubectl exec -it nginx /bin/bash \nkubectl exec -it nginx ls /path/found/when/running/describe/pod\nkubectl get pods --all-namespaces\nkubectl get &lt;item&gt; &lt;itemName&gt; --export -o yaml &gt; exported.yaml\n\n# Deployment Commands\nkubectl create deployment nginx --image=nginx\nkubectl run nginx --image=nginx  --replicas=3\nkubectl get deploy busybox --export -o yaml &gt; exported.yaml\n\nkubectl create job nginx --image=nginx\nkubectl create cronjob nginx --image=nginx --schedule=\"* * * * *\"\n\n# Force Update\nkubectl set image deploy/nginx nginx=nginx:1.9.1\n\n# Rollback Update\nkubectl rollout undo deploy/nginx\nkubectl rollout status deploy/nginx\n\n# Observability (Needs Heapster)\nkubectl top pod -n my-namespace\nkubectl top node -n my-namespace\nkubectl logs -f podName containerName    &lt;- stream logs live, container can be left out\n\n# Service\nkubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml\n\n# Taint \nkubectl taint nodes NodeA app=blue:NoSchedule\n\n# Secrets\nkubectl create secret generic my-secret --from-literal=foo=bar -o yaml --dry-run &gt; my-secret.yaml\n\nkubectl get secret SecretName -o yaml    &lt;- view secret\n</code></pre>"},{"location":"kubernetes/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Flow of Actions</li> <li>Deep Dive: Kube Proxy</li> <li>Deep Dive: Security 4C's</li> <li>Deep Dive: Network Providers</li> </ol> <p>Table of Contents - Control Plane Components - Work Node Components - Flow of Actions - Extending w/ Add-Ons - Additional Components - Deep Dive - Operators - Deep Dive - Kube Proxy - Deep Dive - Network Plugins - Deep Dive - Security (4C's) - Deep Dive - StatefulSets (CassandraDB)</p>"},{"location":"kubernetes/#control-plane-components-case","title":"Control Plane Components (C.A.S.E.)","text":"<p>Contains 4 Total Components 1. API Server 2. etcd 3. Scheduler 4. Controller Manager</p>"},{"location":"kubernetes/#control-plane-details","title":"Control Plane Details","text":"<ul> <li> <p>API Server: communicates with etcd to read and write the desired state. Desired state is provided from user performing kubectl tasks like sending a pod definition in the form of yaml. Responsible for processing requests for creating and updating resources</p> </li> <li> <p>etcd: key-value store used as primary database; stores the desired state of cluster </p> </li> <li> <p>Scheduler: looks for pods in the pending state that have not been assigned to node yet selects them based on factors like resources, afinity rules and which nodes have capacity. once selected, must talk to the <code>kubelet</code> which will then take on the responisibility of the pod</p> </li> <li> <p>Controller Manager: focuses on the controllers that handle specific types of resources and their desired state for <code>reconciliation</code> and <code>corrective</code> purposes. so the replication controller kicks off and ensure the desired pod replicas are maintained</p> </li> </ul>"},{"location":"kubernetes/#worker-node-components-pkr","title":"Worker Node Components (P.K.R.)","text":"<p>Contains a total of 3 Components 1. Kubelet 2. Kube Proxy 3. Container Runtime</p>"},{"location":"kubernetes/#worker-node-details","title":"Worker Node Details","text":"<ul> <li> <p>Kubelet: agent that resides on the node and recieves pod assignment from the scheduler which updates that pod information on itself. From there it communicates with the <code>container runtime</code> to create and manage the containers for the pods. it also monitors the nodes health and status of the containers and restarts/stops/deletes if need be while also reporting node status and resouce usage back to control plane</p> </li> <li> <p>Kube Proxy: maintains network rules on nodes to enable and manage network connectivity to and from pods. so on the nodes, there are network rules typically IPTables (linux based firewall rules for NAT) to define network traffic routing to pods based on service selectors and IP addresses; this also handles when new services are created and handling the forwarding of traffic to the backend pods correctly and even uses the services DNS name alongside the ClusterIP. So the NodePort, ClusterIP, LoadBalancer etc supports the traffic of these. </p> </li> <li> <p>Container Runtime: software responsible for doing the <code>docker run/build</code> commands via Containerd</p> </li> </ul>"},{"location":"kubernetes/#flow-of-actions","title":"Flow of Actions","text":"<p>Goal is always understanding desired state 1. User runs <code>kubectl create pod</code> command to <code>API Server</code> 2. <code>API Server</code> stores this desired state into the <code>etcd</code> store 3. The <code>Scheduling Phases</code> kicks in and watches for <code>pending</code> state pods and evaluates for various factors and selects appropriate node and assigns 4. The <code>Kubelet Phase</code> activates and accepts assignment and interacts with <code>Container Runtime</code> to create the containers for that pod 5. The <code>Kubelet</code> updates status of the containers to <code>etcd</code> in Control Plane 6. The <code>KubeProxy</code> begins to handle the networking </p>"},{"location":"kubernetes/#extending-cluster-functionality-through-common-add-ons","title":"Extending Cluster Functionality through Common Add-Ons","text":"<ol> <li> <p>CoreDNS: handles DNS resolution for pods and services using the following structure <code>serviceName.namespace.svc.cluster.local</code> </p> </li> <li> <p>Dashboard: WebUI for monitoring cluster</p> </li> <li> <p>Ingress Controllers: enables external access to services along with load-balancing</p> </li> <li> <p>Network Policy: not to be confused with network plugin, but this handles traffic flow between pods based on rules</p> </li> <li> <p>Service Mesh: categorized as <code>infrastructure</code> add-ons which enhance networking by adding features like traffic-management, observability and security for microservices</p> </li> <li> <p>Flux/Flagger: categories as <code>tools</code> not add-ons which handle automation of app deployments, manage releases and progressive delivery like canary deployments</p> </li> </ol>"},{"location":"kubernetes/#additional-components","title":"Additional Components","text":"<ol> <li> <p>ConfigMaps: allows to <code>decouple</code> configuration data like <code>API Endpoints</code> and <code>Connection Strings</code> from the pod definitions so you can update pod configurations without actually changing the pod specs</p> </li> <li> <p>Labels and Selectors: Labels are <code>key-value</code> pairs and Selectors <code>filter</code> based on these labels; So you can have a pod with the <code>prod</code> label and apply policies to the selector for that environment</p> </li> <li> <p>Annotations: adds additional metadata so you can track <code>buildIDs</code> or <code>ReleaseNotes</code> </p> </li> <li> <p>StatefulSets: helps with consistency for deploying <code>databases</code> or <code>queues</code> when you need guarantees for consistent network hostnames and stable storage after pod restarts</p> </li> <li> <p>DaemonSets: ensure pod is running on every node which is good for <code>monitoring agents</code> or log collectors; Kured is a DaemonSet</p> </li> <li> <p>Custom Resource Definitions (CRDs): define custom resources and controllers which is good for a Machine Learning Model so Kubernetes can understand and manage it like it was a built in resource</p> </li> </ol>"},{"location":"kubernetes/#kubernetes-deep-dives","title":"Kubernetes Deep Dives","text":""},{"location":"kubernetes/#kubernetes-operators","title":"Kubernetes Operators","text":"<p>What: special type of controller that leverage the use of <code>Custom Resource Definitions</code> to handle complex application configurations</p> <p>Benefits: users can take high level configurations and settings within a <code>CRD</code> and the <code>Operator</code> will watch for that CRD and implement into <code>low-level</code> actions within Cluster. Allows you to focus on defining desired state while the operator will handle the operational task to make it happen!</p> <p>Helm-Chart Comparison: although they have somewhat of the same functionality, Operators focus on complex application deployments due to their integration directly with Kubernetes since they are basically controllers which will handle current/desired state</p>"},{"location":"kubernetes/#kube-proxy","title":"Kube-Proxy","text":"<p>Overview: manages network connectivity for pods &amp;&amp; ensures services are accessible in/outside cluster and ensures they go to healthy pods via health-checks</p>"},{"location":"kubernetes/#key-aspects-of-kube-proxy","title":"Key Aspects of Kube Proxy","text":"<ol> <li> <p>Network Rules Management: since this sits on <code>NODES</code> the network rules are applied via IPTables and define pod traffic routing using their <code>IP Address</code> or <code>Service Selector</code></p> </li> <li> <p>Service Exposure: when a service is created, backend pods need to have that traffic forwards to them. Pods also need to be accessible via DNS name or ClusterIP address</p> </li> <li> <p>Service Types: handles the big 4, ClusterIP, NodePort, LoadBalancer, and ExternalDNS </p> </li> </ol>"},{"location":"kubernetes/#network-plugins","title":"Network Plugins","text":"<ol> <li> <p>Overlay Networks: virtual networks built on-top of physical infrastructure; biggest value is they allow <code>pods on different nodes</code> to talk to each other as if they were on <code>the same local network</code> even if they are in different physical locations</p> </li> <li> <p>Pod to Node Commuication: typically this is done using <code>encapsulation</code> techniques. If a pod on NodeA wants to talk to pod on NodeB, data packets are encapsulated with additional information like <code>source and destination IP addresses</code> and are sent over network and <code>decapsulated</code> thereafter</p> </li> <li> <p>Scalability: as you add more clusters, nodes, or pods to cluster, the network plugins dynamically manage IP address allocation and routing keeping everything isolated </p> </li> </ol> <p>Network Provider - Flannel - Calico</p> <p>Flannel - uses an overlay network which makes things simple - less advanced features - not ideal for organizations that have strict security requirements BUT you can add in other projects to improve security - incurs extra encapsulation which slows down communications due to increased size of packets because it has information in the packet that is necessary for reaching the destination. furthermore, increased CPU resources due to encapsulation/decapsulation so latency is increased which is not good for high network traffic environments</p> <p>Calico - does not use a overlay network, which improves performance and scalability - uses BGP to route packets between host so they dont need an extra layer wrapped with encapsulation to move between hosts - directs packets natively without an extra step of wrapping traffic in an additional layer - </p>"},{"location":"kubernetes/#understanding-overlay-networks","title":"Understanding Overlay Networks","text":"<p>Purpose: allows pods and nodes to communicate regardless if they are hosted on different machines and subnets in a cluster</p> <p>Components: - Encapsulation: traffic is wrapped with the sourceNode and decapsulated at the <code>destinationNode</code> - Routing/Forwarding: decisions are done with the overlay network NOT with the physical network. Overlay will have its own routeTables and protocols to direct traffic</p>"},{"location":"kubernetes/#understanding-networking-traffic","title":"Understanding Networking Traffic","text":""},{"location":"kubernetes/#flannel-traffic-flow-example","title":"Flannel - Traffic Flow Example","text":"<p>Steps: 1. ContainerA tries to talk to ContainerB on a different Host 2. ContainerA traffic goes to HostA's bridge 3. HostA Bridge tries to get the MAC address of ContainerB via ARP 4. Since ContainerB is on HostB, flannel daemon wraps into Layer3 over UDP over physical network</p>"},{"location":"kubernetes/#calico-traffic-flow-example","title":"Calico - Traffic Flow Example","text":"<p>Steps: 1. Already works at Layer3 2. There is a routing rule for a default gateway of 169.254.1.1 3. ContainerA talks to default gateway FIRST 4. Because of the BGP client running on the Node, it is synced and knows how to route to the destination</p>"},{"location":"kubernetes/#differences","title":"Differences","text":"<ul> <li>Calico makes use of routing principals like how the internet works </li> <li>Flannel makes use of L2 broadcast domains</li> </ul>"},{"location":"kubernetes/#security-4cs","title":"Security - 4C's","text":"<ol> <li>Code: Secure apps and libraries</li> <li>Container: Vulnerability Scanning, Control and Protect Runtime</li> <li>Cluster: restrict access to Control Plane and Nodes</li> <li>Cloud: Secure Access to API's and Infrastructure (baremetal)</li> </ol>"},{"location":"kubernetes/#statefulsets-and-cassandradb","title":"StatefulSets and CassandraDB","text":"<p>When creating a StatefulSet, there are multiple components that are deployed or needed as well. - Pods get stable hostnames and ordinal index ie pod01, pod02 - A headless service (no ClusterIP) is deployed to discover individual pods via DNS - StorageClass and PVC's are deployed which bind persistent storage to pods even if they are replaced</p> <p>CassandraDB: because of its distributed nature, it uses sharding so data is distributed amongst each other. Benefit: although using a DB hosted elsewhere is useful, when you need local data or to keep it closer to the apps, integration with K8 ecosystem like rolling updates or scaling and of course the scaling and availability of K8s, this is uesful.</p>"},{"location":"kubernetes/#storage-in-k8s-important-concepts","title":"Storage in K8s Important Concepts","text":"<p>Options 1. Leverage PV and PVC's via HostPath 2. StatefulSets 3. StorageClass w/ Dynamic Provisioning</p> <p>Important Notes - Storage Classes allow for pods to claim a PV even if there is no match - PVC's consume PV resources like Pods do Nodes</p> <p>Storage Classes - helps to define different types of storage that can be leveraged (SSD,HDD) - allows for dynamic provisioning if PVC's are leveraged - defines provisioners for dynamic provisioning Azure Disk, Nutanix or distribed storage like Ceph</p> <p>Considerations for Storage Classes - on-demand volumes to be created without admins via dynamic provisoning - if you need different types of storage for faster or slower apps - small environments this is NOT needed</p>"},{"location":"kubernetes/#statefulset-blog","title":"StatefulSet Blog","text":"<p>Interesting Links: - Stateful Best Practices Blog - Stateful Redis Guestbook App Link - Stateful WordPress and mySQL Example Link</p>"},{"location":"kubernetes/#research-topics","title":"Research Topics","text":"<ul> <li>Primary-replica architecture + fixed pod-name being expected for stateful Applications</li> </ul>"},{"location":"kubernetes/#understanding-stateful-applications","title":"Understanding Stateful Applications","text":"<p>What: - Stateful Applications: apps that need to store data and keep track of it - Examples of StatefuApps: databases like mySQL, Oracle are statefulApps - Stateless Apps: these are apps that during each new request, they get <code>new</code> data and process it like Nginx and NodeJS</p> <p>Modern Architecture: Stateless Application <code>NodeJS</code> --connects to --&gt; StatefulApplication <code>mySQL</code></p>"},{"location":"kubernetes/#understanding-statefulsets","title":"Understanding StatefulSets","text":"<p>StatefulSet: controller that is used to run the stateful app as a <code>pod</code> - assigns a <code>sticky identity or ordinal number</code> like <code>pod[0]</code> to each replicaPod vs <code>deployments</code> which get randomIDs - new pods are created by cloning the previous pod's data  - deleting pods will be done in <code>reverse order</code> so scaling from 4 to 3 will delete the most recent created pod vs randomly</p>"},{"location":"kubernetes/#statefulset-readwrite-process","title":"StatefulSet - Read/Write Process","text":"<p>Reading Data: request is forwarded to any of the 3 pods Writing Data: request will be forwarded to <code>pod[0]</code> which is the primary and then synced to other pods</p>"},{"location":"kubernetes/#storage-class-example","title":"Storage Class Example","text":"<pre><code>#Example Storage Class\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: slow\nprovisioner: kubernetes.io/no-provisioner\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre> <p>Persistent Volumes - Storage Classes tend to create PVs - an additional resource as a volume plugin provisioned by the Cluster Admin</p> <p>Persistent Volume Claims - requested by the user via the deploy.yaml - PVC's consume PV resources like a Pod consumes Node resources - Access modes like RWO or RWM determine capabilities  - K8 Control Plane will look to match the PVC to the PV. BUT if notExists + Dynamic Provisioning is enabled via StorageClass it will be created regardless</p>"},{"location":"kubernetes/#options","title":"Options","text":"<p>To ensure data persistence in Kubernetes, you would typically use a Persistent Volume (PV) and a Persistent Volume Claim (PVC). A Persistent Volume is a piece of storage that has been provisioned by an administrator, while a Persistent Volume Claim is a request for storage by a user. Additionally, you might consider using StatefulSets if your application requires stable, unique network identifiers, stable, persistent storage, and orderly, graceful deployment and scaling.</p> <p>Here are the different options you could use to ensure data persistence for your Python Docker container within a Kubernetes cluster:</p> <ol> <li>Persistent Volumes and Persistent Volume Claims:</li> <li>First, you create a Persistent Volume (PV) that provides the storage resource.</li> <li>Next, you create a Persistent Volume Claim (PVC) which claims usage of the PV for your application.</li> <li>Finally, you modify your application's deployment configuration to use the PVC.</li> </ol> <p>Here's how you could do it:</p> <pre><code># persistent-volume.yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: countit-pv\nspec:\n  capacity:\n    storage: 1Gi  # adjust the size as needed\n  accessModes:\n    - ReadWriteOnce\n  hostPath:  # for demonstration purposes, not suitable for production\n    path: \"/mnt/data\"\n---\n# persistent-volume-claim.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: countit-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n---\n# deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fastapi-counter\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: fastapi-counter\n  template:\n    metadata:\n      labels:\n        app: fastapi-counter\n    spec:\n      containers:\n      - name: fastapi-counter-container\n        image: &lt;your-image-name&gt;\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: countit-storage\n          mountPath: /usr/src/app\n      volumes:\n      - name: countit-storage\n        persistentVolumeClaim:\n          claimName: countit-pvc\n</code></pre> <ol> <li>StatefulSets with Persistent Volume Claims:</li> <li>StatefulSets are used when you have stateful applications that require stable identifiers and storage.</li> </ol> <pre><code># statefulset.yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: fastapi-counter\nspec:\n  serviceName: \"fastapi-counter\"\n  replicas: 1\n  selector:\n    matchLabels:\n      app: fastapi-counter\n  template:\n    metadata:\n      labels:\n        app: fastapi-counter\n    spec:\n      containers:\n      - name: fastapi-counter-container\n        image: &lt;your-image-name&gt;\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: countit-storage\n          mountPath: /usr/src/app\n  volumeClaimTemplates:\n  - metadata:\n      name: countit-storage\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 1Gi\n</code></pre> <ol> <li>Using a Storage Class:</li> <li>Storage Classes provide a way to describe different \u201cclasses\u201d of storage offered in the cluster.</li> </ol> <pre><code># storage-class.yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fastapi-counter-sc\nprovisioner: kubernetes.io/no-provisioner\nvolumeBindingMode: WaitForFirstConsumer\n---\n# Then you can reference this Storage Class in your Persistent Volume and Persistent Volume Claim configurations.\n</code></pre> <p>Replace <code>&lt;your-image-name&gt;</code> with the name of your Docker image. Each of these options has its own set of considerations and trade-offs, so you'll need to choose based on your application's requirements and your environment.</p>"},{"location":"kubernetes/#nutanix-notes-for-storage-not-finished","title":"Nutanix Notes for Storage (Not Finished)","text":"<ul> <li>RWO (Nutanix Volumes) vs RWM (Nutanix Files) or others</li> <li>relationship between pods and PVC or PV's, what gets x and why?</li> </ul> <p>Simple Example LI5 - Storage Classes: different types of boxes - PV: the actual box being granted - PVCs: tickets asking for the box - StatefulSet: already reserved box regular users get everytime </p> <p>Persisting Data on Nutanix Kubernetes 0. Create the Nutanix StorageClass (not 100% reqired since spec.storageClass if empty will default to pvc without <code>dynamic provisioning</code> being used) 1. Create PV that provides storage 2. Create PVC that uses the PV created already 3. Pod or Deploy yaml will mount the volume referencing the PVC</p>"},{"location":"kubernetes/#create-pv","title":"Create PV","text":"<p>Note: Need to use the correct Nutanix specs for this. This will be the spec.storageClassName which will reference the provider <pre><code># persistent-volume.yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: countit-pv\nspec:\n  capacity:\n    storage: 1Gi  # adjust the size as needed\n  accessModes:\n    - ReadWriteOnce\n  hostPath:  # for demonstration purposes, not suitable for production\n    path: \"/mnt/data\"\n</code></pre></p>"},{"location":"kubernetes/#create-pvc","title":"Create PVC","text":"<p>Note: </p> <pre><code># persistent-volume-claim.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: countit-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre>"},{"location":"kubernetes/#sample-deploymentyaml","title":"Sample Deployment.yaml","text":"<p>Note: spec.volumes will point to the pvc.claimName. Since the app source data lives in the /usr/src/app folder, mount it there to hold the required files</p> <pre><code># deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fastapi-counter\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: fastapi-counter\n  template:\n    metadata:\n      labels:\n        app: fastapi-counter\n    spec:\n      containers:\n      - name: fastapi-counter-container\n        image: &lt;your-image-name&gt;\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: countit-storage\n          mountPath: /usr/src/app\n      volumes:\n      - name: countit-storage\n        persistentVolumeClaim:\n          claimName: countit-pvc\n</code></pre>"},{"location":"kubernetes/#persistent-storage-with-nginx","title":"Persistent Storage with Nginx","text":"<p>Mounting Volumes vs Attaching Local Directory - Mounting: this is helpful when you have an application that will perform some CRUD actions - Attaching: more useful when you are developing real-time and need to test. Think of this as <code>dev-containers</code></p>"},{"location":"kubernetes/#table-of-contents_1","title":"Table of Contents","text":"<ul> <li>Simple with Custom Starting Page</li> <li>Bi-Directional Development with Volume Mount to Local</li> <li>Attaching Volumes for CRUD - Sample FastAPI</li> <li>Transitioning to Kubernetes</li> </ul>"},{"location":"kubernetes/#option-1-create-a-simple-nginx-custom-starting-page","title":"Option 1: Create a Simple Nginx Custom Starting Page","text":"<p>Purpose: - You require static files to be ready when container loads - Regardless of pod restarts, data will persists</p> <p>Steps 1. Create a simple <code>custom.html</code> page 2. Build a simple Dockerfile and store html into defaut directory 3. Test</p>"},{"location":"kubernetes/#1-create-simple-html-file","title":"1. Create Simple HTML File","text":"<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Custom Nginx Page&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Welcome to my custom Nginx page!&lt;/h1&gt;\n    &lt;h3&gt;Version v1&lt;/h3&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"kubernetes/#2-build-dockerfile-and-move","title":"2. Build Dockerfile and Move","text":"<p>Create <code>Dockerfile</code> <pre><code>FROM nginx\n\nCOPY custom.html /usr/share/nginx/html/index.html\n\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n</code></pre></p>"},{"location":"kubernetes/#3-testing","title":"3. Testing","text":"<p>Build Container: <code>docker build -t custom-nginx:v1</code></p> <p>Run Container: <code>docker run -d -p 80:80 custom-nginx:v1</code></p> <p>Navigate to <code>localhost:80</code></p>"},{"location":"kubernetes/#option-2-mounting-and-exposing-volumes-in-containers","title":"Option 2: Mounting and Exposing Volumes in Containers","text":"<p>Purpose: - When you require access to local storage - Perfect for Real-Time Development</p> <p>Steps 1. Know the path of the local directory you would like to mount 2. Create a temp file called <code>index.html</code> 3. Pass in the location in the docker run command</p>"},{"location":"kubernetes/#2-pass-in-location-in-docker-command","title":"2. Pass in Location in Docker Command","text":"<p>Run Container: <code>docker run -d -p 80:80 --name tempcustom-nginx -v /home/devops/nginx:/usr/share/nginx/html nginx</code></p> <p>Example <code>Dockerfile</code> <pre><code>FROM nginx\n\nCOPY ./custom-nginx-html /usr/share/nginx/html\n\nEXPOSE 80\n\n# Note: you can change the /etc/nginx/conf.d file to change the index file name. Mount this before because it requires a restart\n</code></pre></p>"},{"location":"kubernetes/#option-3-volumes-attaching-w-fastapi","title":"Option 3: Volumes Attaching w/ FastAPI","text":"<p>Steps 1. Create simple .py app that will have an /increment endpoint and add to a file we can /get anytime 2. Build the Dockerfile 3. Create and Ensure Volume is attached to running container</p>"},{"location":"kubernetes/#1-create-mainpy","title":"1. Create <code>main.py</code>","text":"<pre><code>from fastapi import FastAPI\nfrom pathlib import Path\n\napp = FastAPI()\n\nCOUNTER_FILE = Path(\"/usr/src/app/counter.txt\")\n\ndef read_counter():\n    if COUNTER_FILE.exists():\n        return int(COUNTER_FILE.read_text())\n    return 0\n\ndef write_counter(count):\n    COUNTER_FILE.write_text(str(count))\n\n@app.get(\"/increment/\")\ndef increment_counter():\n    count = read_counter()\n    count += 1\n    write_counter(count)\n    return {\"count\": count}\n\n@app.get(\"/get/\")\ndef get_counter():\n    return {\"count\": read_counter()}\n</code></pre>"},{"location":"kubernetes/#2-create-dockerfile","title":"2. Create Dockerfile","text":"<pre><code># Use the official Python image as the base image\nFROM python:3.9-slim-buster\n\n# Set the working directory\nWORKDIR /usr/src/app\n\n# Copy the FastAPI application\nCOPY ./main.py .\n\n# Install FastAPI and Uvicorn\nRUN pip install fastapi uvicorn\n\n# Expose the port the app runs on\nEXPOSE 80\n\n# Command to run the application\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n</code></pre>"},{"location":"kubernetes/#3-configure-docker","title":"3. Configure Docker","text":"<p>What Happens When You Dont Attach Volume? - Run command: <code>docker run -d -p 80:80 counterapp:v1</code> - Output: Application works <code>HOWEVER</code> when you restart the container, the counter /get does not persist the data</p> <p>Create Volume: <code>docker volume create countit</code> Attach Volume: <code>docker run -d -p 80:80 --name fastapi-counter-container --volume countit:/usr/src/app fastapi-counter</code> Note: you can also use the VOLUME command in the Dockerfile</p>"},{"location":"kubernetes/#option-4-moving-to-kubernetes","title":"Option 4: Moving to Kubernetes","text":"<p>Options 1. Leverage PVs and PVCs via HostPath 2. Leverage StatefulSets 3. Leverage StorageClass Dynamic Provisioning</p>"},{"location":"mlops/","title":"MLOps","text":""},{"location":"mlops/#ml-on-kubernetes-with-kubeflow","title":"ML on Kubernetes with KubeFlow","text":""},{"location":"mlops/#building-ml-apps-overview-and-challenges","title":"Building ML Apps Overview and Challenges","text":"<p>Machine Learning Enabled Applications do not have linear workflow. This means that the stages of research and development as well as production are iterative for the life cycle.</p> <ol> <li> <p>Research: the initial machine learning models are created and datasets are explored</p> </li> <li> <p>Development: this is where initial models are refined, tested and tured into more production ready solutions</p> </li> <li> <p>Production: models are deployed into real-world applications to process live data</p> </li> <li> <p>Post-Production: where monitoring performance and retraining with new data is important which returns back to Research stage.</p> </li> </ol> <p>Using CI/CD helps manage complexity by providing structure into merging, testing and delivering code changes to production systems that can be done in an automated and reliable way. This helps ensure that applications remain stable and can rapidly adapt to new requirements or data. </p>"},{"location":"mlops/#introducing-containers","title":"Introducing Containers","text":"<p>Since containers do not require the entire OS, only the immediate components it needs to operate, this can improve the speed of development as developers know that the application will run the same way on different platforms. This allows developers to focus on their apps and operations can focus on instrastructure. This allows for integrating new code into an application so it can grow and mature through its lifecycle.</p>"},{"location":"mlops/#container-less-environment-challenges","title":"Container-less Environment Challenges","text":"<p>Although MLOps and these capabilities can be solved without containers, there are many challenges developers and infrastructure teams will face. </p> <p>There is the option of <code>Virtual Machines</code> as a Partial Solution because the application can run with all dependancies in a self-contained environment BUT due the resource usage and overhead of managing full OS's which can lead to <code>under-utilization</code> of resources and <code>longer startup</code> times.</p> Challenges Details Dev Environment Inconsistency \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 As Developers build code on their local machines, this leads to inconsistent setups and configurations Dependency Management As the application grows, new code might require new libraries at specific versions which forces all servers (testing and prod), local dev machines etc to be installed for the code to actually run properly Deployment Overheads Need to deploy a new application? This would require coodination with the system admin or ops to manually setup the server environment to match the dev's environment. Each change could potentially lead to downtime.   Containers are deployed as simple as pulling and building the container image on the host that has the Docker Runtime installed. Configuration Drift This is common to \"get things to work\" where adhoc changes in a production environment to resolve issues or update components. This makes it harder to replicate issues or test new features due to the drift. Scaling Challenges This tends to require cloning production environments and setting this back up on new servers which is time consuming and can be error proned. You then need to load-balance and redistribute traffic to these new servers and new challenges might arise which could require manual configuration.   Containers can be quickly started and stopped and even replicated."},{"location":"mlops/#kubernetes-value-with-aiml-workloads","title":"Kubernetes Value with AI/ML Workloads","text":"<p>Tools like <code>KubeFlow</code> help simply the process of training and deploying ML models at scale to standardize the idea of <code>MLOps</code>. </p> <ol> <li> <p>Scalability: ML pipelines can accomodate large-scale processing and training without intefereing with other project elements</p> </li> <li> <p>Efficiency: K8s will handle resource allocation by scheduling workloads on nodes based on availability and capacity. This insures that resources are being utilized with intention</p> </li> <li> <p>Portability: allows for developing one ML model and deploy across multiple environments making it agnostic for any platform</p> </li> <li> <p>Fault Tolerance: has self-healing capabilities to ensure K8's keeps ML pipelines running in the event of failures</p> </li> </ol>"},{"location":"mlops/#what-does-mlops-look-like-without-kubeflow","title":"What does MLOps look like without Kubeflow?","text":"<p>Without Kubeflow or other Kubernetes-based platforms and tools, managing the machine learning lifecycle would be more complex and could look something like this:</p> <ol> <li> <p>Development Stage:</p> <ul> <li>Developers write ML code on their local machines, which may have different environments and dependencies.</li> <li>Sharing code across a team is challenging, and reproducing results is difficult due to environment discrepancies.</li> <li>Scaling experiments from a local machine to a larger cluster is a manual and error-prone process.</li> </ul> </li> <li> <p>Testing Stage:</p> <ul> <li>Rigorous testing of models would require setting up multiple environments to simulate different operating systems or configurations.</li> <li>There's no automated way to ensure that the code changes will work across all these environments, leading to potential failures after deployment.</li> </ul> </li> <li> <p>Deployment and Production Stage:</p> <ul> <li>Deploying an ML model into production involves transferring from a development environment to a server environment, which might not match.</li> <li>Scaling the model to handle more data or more requests would typically require manual replication of the model setup on new servers.</li> <li>Load balancing, rolling updates, and version control of ML models in production are done manually.</li> </ul> </li> <li> <p>Portability and Scalability:</p> <ul> <li>Moving an ML application from one cloud provider to another, or from cloud to on-premises, would require significant effort due to the tight coupling with the underlying infrastructure.</li> <li>Scaling up and down to handle variable workloads is not automated and requires manual intervention, which could lead to inefficiencies and high operational costs.</li> </ul> </li> <li> <p>Infrastructure Management:</p> <ul> <li>Managing the underlying infrastructure for different ML tools and frameworks is done manually by IT and operations teams.</li> <li>Each change in the infrastructure might affect the running ML applications, so careful coordination is required between teams.</li> </ul> </li> <li> <p>Operational Overhead:</p> <ul> <li>Each team must manage their own updates and security patches for all different environments, creating additional operational overhead.</li> <li>Without a standardized application packaging system, collaboration and troubleshooting become more difficult.</li> </ul> </li> </ol> <p>In summary, without a platform like Kubeflow, the ML lifecycle would be characterized by fragmented development practices, inefficient resource utilization, operational challenges in maintaining consistency across environments, and difficulty in managing the orchestration of ML workflows at scale.</p> <p>Challenges Kubeflow Solves for MLOps:</p> <ul> <li> <p>Standardization: Kubeflow simplifies the operationalization of ML workflows by providing a standard, unified platform that works across various environments.</p> </li> <li> <p>Automation: With Kubernetes, many of the manual deployment and scaling processes are automated. Kubeflow builds on this to automate ML pipelines, from data preprocessing to model training and serving.</p> </li> <li> <p>Scalability: Kubeflow leverages Kubernetes' ability to scale resources efficiently and automatically according to the demands of the workload.</p> </li> <li> <p>Portability: Kubeflow makes it easier to move ML workloads between environments\u2014cloud, on-premises, or hybrid\u2014without needing significant changes.</p> </li> <li> <p>Reproducibility: Kubeflow helps ensure that ML experiments are reproducible, with a consistent environment from development to production.</p> </li> <li> <p>Resource Utilization: Kubernetes optimizes the use of underlying hardware, and Kubeflow manages the ML-specific resource requirements.</p> </li> </ul> <p>By explaining these challenges and showcasing how Kubeflow helps overcome them, you can make a compelling case for adopting Kubeflow and Kubernetes in managing ML workloads. The efficiency, agility, and cost benefits provided by these technologies are strong motivators for organizations looking to streamline their MLOps practices.</p>"}]}